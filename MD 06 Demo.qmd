---
title: "Chapter 06 Demo"
author: "Logan Agin"
format: html
editor: visual
---

Load packages and set theme.

```{r}
#| message: false # this needs to be in all your homeworks from now on 

library(tidyverse)

library(moderndive)

library(ggthemes)

library(patchwork)

theme_set(theme_light()) #visuals, it needs to look nice
```

#\| message: false

library(tidyverse)

library(moderndive)

library(ggthemes)

library(patchwork)

theme_set(theme_light())

\`\`\`

Let's look at the MD evaluations data.

```{r}
data(evals)

glimpse(evals)
```

\`\`\`

We are going to ask whether the "beauty" of an instructor predicts their teaching evaluations.

```{r}
d <- evals |>

  rename(bty = bty_avg,    # just shorter to type

         sex = gender)     # actually what they have

glimpse(d)
```

Let's look at the first few cases.

```{r}
head(d)
```

## Here's the regression from last week.

```{r}
mod1 <- lm(score ~ bty,

           data = d)

get_regression_table(mod1)
```

Int = 3.88 ( at y = 0 , x = 3.88)

slope = .067

what this says : every time you go up by 1 point on the x the y will increase by .067 starting at 3.88

so if beauty point is 7 then 3.88+7(.067) (y= mx + b)

## Let's look at the predictions and residuals.

```{r}
mod1_preds <- get_regression_points(mod1)

head(mod1_preds)  # creating a table of actual, expected and resdiuals
# can use it to make a plot using ggplot(mod1_preds, aes(...))
```

4.7 actually happen

4.214 expected

residual is different of actual - expected (4.7 - 4.214)

\`\`\`

## Regression line (blue).

```{r,echo=FALSE}
ggplot(d,

       aes(x = bty,

           y = score)) +

  geom_jitter(alpha = .3) +

  geom_smooth(method = "lm",

              se = FALSE) +

  labs(x = "Beauty",

       y = "Evaluation",

       title = "Simple regression results")
```

\`\`\`

## Residuals

```{r,echo=FALSE}
ggplot(mod1_preds, 

       aes(x = bty,

           y = residual)) +

  geom_jitter(alpha = .3) +

  geom_hline(yintercept = 0,

             color = "blue") +

  labs(x = "Beauty",

       y = "Residual",

       title = "Simple regression residuals")
```

formula for residuals = Yi - Y\^ i (hat or \^ is an estimate)

look at residuals to check your line of best fit in regression plot

blue line is guessing the average score for everyone (for this plot about 4.1)

the points show how far off our average predicted score is for that point

\`\`\`

One way to quantify how well a predictor predicts the outcome is to use the \*\*variance\*\*. We've seen this already but let's review. Here's the formula. \$\\bar{y}\$ is the mean of \$y\$.

\$\$

V(y) = \\frac{1}{n-1}\\sum\_{i=1}\^{n}(y_i - \\bar{y})\^2

\$\$

## Since our outcome is evaluation score, we can just do that.

```{r}
var_y <- d |> #take the data set

  pull(score) |> #pull a column out and treat it as a vector 

  var() #calculate the list of variants in the column 

var_y 
```

\`\`\`

This is equivalent to the variance of the "residuals" when we just guess the mean value for every person!

```{r, echo=FALSE}
ggplot(d,

       aes(x = bty,

           y = score)) +

  geom_jitter(alpha = .3) +

  geom_hline(yintercept = mean(d$score),

             color = "blue") +

  labs(x = "Beauty",

       y = "Evaluation",

       title = "Guessing the mean for everyone")
```

idea here is to see if beauty of the instructor impacting the evaluation score

\`\`\`

## Adding \`beauty\` as a predictor improves our guess.

```{r, echo=FALSE}
ggplot(d,

       aes(x = bty,

           y = score)) +

  geom_jitter(alpha = .3) +

  geom_hline(yintercept = mean(d$score),

             color = "blue") +

  geom_smooth(method = "lm",

              se = FALSE,

              color = "red",

              linetype = "dashed") +

  labs(x = "Beauty",

       y = "Evaluation",

       title = "Mean vs. regression line")
```

idea here is to see if beauty of the instructor impacting the evaluation score

is red line a better guess than blue line

## Now let's see what the spread looks like if we look at the residuals from the regression line.

```{r}
var_yhat1 <- mod1_preds |> 

  pull(residual) |> 

  var()

var_yhat1
```

these guesses are going to be smaller, in this we are making better guesses for each indicuial based off beauty instead of an underlining estimate for everyone

\`\`\`

If we take the ratio of these and subtract it from one, that gives us the \$R\^2\$ or "proportion of variance explained" by beauty scores.

```{r}
1 - (var_yhat1 / var_y)
```

how much lower is your better guess variance vs underlining variance? = .0349

so, using beauty to make a better guess we get a 3.5% difference than overall average guess

It looks like "beauty" can account for about \`r round((1 - (var_yhat1 / var_y)) \* 100, 1)\` percent of the variance in the evaluation scores.

In other words, if we try to guess instructors' evaluation scores, our guesses will be \`r round((1 - (var_yhat1 / var_y)) \* 100, 1)\` percent \_less wrong\_ on average if we know the instructor's beauty rating.

We can get this from \`broom::glance()\` or \`moderndive::get_regression_summaries()\` without doing it manually. The latter will be a more curated list of things you'll need for this course.

```{r}
broom::glance(mod1)

moderndive::get_regression_summaries(mod1) # use this one, simpler 
```

start thinking of this like an algebra problem

will help you solve

\`\`\`

Let's try a different predictor. Let's predict ratings with \`sex\`.

```{r}
mod2 <- lm(score ~ sex,

           data = d)

get_regression_table(mod2)

get_regression_summaries(mod2)
```

int = 4.093

slope = .142

4.093 + M or F (.142)

\`\`\`

Looks like male instructors get slightly better evaluations but this only accounts for a tiny bit of the of the variance. Don't be confused however: small amounts of variance can be really important even if they don't tell the whole story of variability.

In general, we are not going to want to do this with different variables one by one. We want to do \*\*multiple regression\*\*.

```{r}
mod3 <- lm(score ~ bty + sex,

           data = d)

get_regression_table(mod3)
```

looking to see if the beauty of a male or female is accounting for an increase or decease in their evaluation score

int = 3.747

bty slope = .074

male sex = .172

\`\`\`

Here's what this model does.

```{r, echo=FALSE}
ggplot(d,

       aes(x = bty,

           y = score,

           group = sex,

           color = sex)) +

  geom_jitter(alpha = .3) +

  geom_parallel_slopes(se = FALSE) +  # this is a modern dive thing!

  theme(legend.position = "top")
```

this shows that the male has a .172 increase in best fit line, saying that males tend to have a beauty score .172 higher than females

\`\`\`

Here's the formula:

\$\$

\\widehat{score}\_i = 3.75 + .074(beauty_i) + .172(male_i)

\$\$

We might instead prefer a model like this, which allows the relationship between beauty and evaluations to be \*different\* for male and female instructors.

```{r, echo=FALSE}
ggplot(d,

       aes(x = bty,

           y = score,

           group = sex,

           color = sex)) +

  geom_jitter(alpha = .3) +

  geom_smooth(method = "lm",

              se = FALSE) +

  theme(legend.position = "top")
```

\`\`\`

Here's the syntax, results, and formula.

```{r}
mod4 <- lm(score ~ bty + sex + bty:sex,

           data = d)

get_regression_table(mod4)  
```

mod4 \<- lm(score \~ bty + sex + bty:sex,

data = d)

get_regression_table(mod4)

\`\`\`

\$\$

\\widehat{score}\_i = 3.95 + .031(beauty_i) + -.184(male_i) + .080(beauty_i \\times male_i)

\$\$

The slope for female instructors is .031. The slope for male instructors is .031 + .080 = .111.

\`\`\`{r}

get_regression_summaries(mod3) \# parallel

get_regression_summaries(mod4) \# interactions

\`\`\`

Do you think this is an important improvement?
